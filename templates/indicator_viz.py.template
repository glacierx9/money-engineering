#!/usr/bin/env python3
"""
# {{NAME}} Indicator Analysis Dashboard

This script provides comprehensive analysis and visualization of the
{{NAME}} indicator, fetching calculated data from the server.

## Features:
- Async data fetching using svr3 module
- Automatic field detection from StructValues
- Interactive time series visualization
- Statistical analysis and distribution plots
- Compatible with interactive and regular Python modes

## Usage:
- Interactive mode (VS Code/Cursor): Run cells directly with await
- Regular mode: python {{NAME}}_viz.py

Reference: Chapter 10 - Visualization
"""

# %% [markdown]
# # {{NAME}} Indicator Analytics Dashboard
#
# Fetches and analyzes {{NAME}} indicator data from server

# %%
# Core imports
import asyncio
import warnings
from typing import Dict, List, Optional
import json

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import svr3

warnings.filterwarnings("ignore")

# Set up plotting style
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")

print("üìä {{NAME}} Indicator Dashboard Initialized")

# %% [markdown]
# ## Configuration
#
# Server connection and date range settings

# %%
# Server configuration
RAILS_URL = "https://10.99.100.116:4433/private-api/"
WS_URL = "wss://10.99.100.116:4433/tm"
TM_MASTER = ("10.99.100.116", 6102)
TOKEN = "YOUR_TOKEN_HERE"  # TODO: Replace with your actual token

# Date range (YYYYMMDDHHmmss format)
START_DATE = 20230103203204
END_DATE = 20230510203204

# Indicator configuration
INDICATOR_NAME = "{{NAME}}"
GRANULARITY = 900  # 15-minute (adjust based on your indicator's output granularity)
NAMESPACE = "private"

# Commodity to visualize (Tier-1: pick ONE commodity; Tier-2: use placeholder)
MARKET = "DCE"          # Adjust based on your uout.json
COMMODITY = "i<00>"     # Adjust based on your uout.json

# For Tier-2 composites, use placeholder from uout.json:
# MARKET = "DCE"
# COMMODITY = "COMPOSITE<00>"

# %% [markdown]
# ## Data Fetcher Class
#
# Handles server connection and data retrieval using svr3

# %%
class {{NAME}}DataFetcher:
    """
    Data fetcher for {{NAME}} indicator using svr3 module

    Implements proper connection lifecycle:
    - connect() ‚Üí login ‚Üí connect ‚Üí ws_loop ‚Üí shakehand
    - fetch() ‚Üí save_by_symbol()
    - close() ‚Üí stop ‚Üí join

    Reference: WOS Chapter 10 - Pattern 2 (connection reuse)
    """

    def __init__(self, token: str, start: int, end: int):
        self.token = token
        self.start_date = start
        self.end_date = end
        self.client = None
        self.df = None
        self.available_fields = []

    async def connect(self):
        """Establish connection to server (call once)"""

        print(f"üîÑ Connecting to server...")

        self.client = svr3.sv_reader(
            self.start_date,
            self.end_date,
            INDICATOR_NAME,
            GRANULARITY,
            NAMESPACE,
            "symbol",
            [MARKET],
            [COMMODITY],
            False,                  # No persistent file
            RAILS_URL,
            WS_URL,
            "",                     # Username (blank)
            "",                     # Password (blank)
            TM_MASTER,
        )
        self.client.token = self.token

        # Connection lifecycle
        await self.client.login()
        await self.client.connect()
        self.client.ws_task = asyncio.create_task(self.client.ws_loop())
        await self.client.shakehand()

        print(f"‚úì Connected to server")

    async def fetch(self, market: str, code: str) -> pd.DataFrame:
        """
        Fetch data for specified market/code

        Args:
            market: Market identifier (e.g., "DCE", "SHFE")
            code: Commodity code (e.g., "i<00>", "cu<00>")

        Returns:
            DataFrame with all indicator fields
        """

        print(f"üìä Fetching {market}/{code}...")

        # Update markets/codes and reuse connection
        self.client.markets = [market]
        self.client.codes = [code]

        # Fetch data - returns List[Dict]
        ret = await self.client.save_by_symbol()
        data = ret[1][1]  # Extract List[Dict] from result tuple

        if not data:
            print(f"‚ö† No data returned for {market}/{code}")
            return pd.DataFrame()

        # Convert List[Dict] to DataFrame
        # Each dict contains header fields (time_tag, granularity, market, code, namespace)
        # plus all fields from uout.json
        df = pd.DataFrame(data)

        # Store available fields (exclude header fields)
        header_fields = ['time_tag', 'granularity', 'market', 'code', 'namespace']
        self.available_fields = [col for col in df.columns if col not in header_fields]

        # Convert time_tag (unix ms) to datetime
        if 'time_tag' in df.columns:
            df['datetime'] = pd.to_datetime(df['time_tag'], unit='ms')
            df = df.sort_values('datetime')

        self.df = df

        print(f"‚úì Loaded {len(df)} data points")
        if 'datetime' in df.columns:
            print(f"  Date range: {df['datetime'].min()} to {df['datetime'].max()}")
        print(f"  Available fields: {', '.join(self.available_fields[:5])}...")

        return df

    async def close(self):
        """Clean up connection"""

        if self.client:
            self.client.stop()
            await self.client.join()
            print("‚úì Connection closed")

    def get_summary(self) -> Dict:
        """Get summary statistics of the indicator data"""

        if self.df is None or self.df.empty:
            return {}

        summary = {
            'total_points': len(self.df),
            'fields': self.available_fields,
        }

        if 'datetime' in self.df.columns:
            summary['date_range'] = (self.df['datetime'].min(), self.df['datetime'].max())

        # Add basic statistics for numeric fields
        numeric_fields = self.df.select_dtypes(include=[np.number]).columns
        summary['statistics'] = {}
        for field in numeric_fields:
            if field not in ['timetag', 'bar_index']:
                summary['statistics'][field] = {
                    'mean': self.df[field].mean(),
                    'std': self.df[field].std(),
                    'min': self.df[field].min(),
                    'max': self.df[field].max()
                }

        return summary

# %% [markdown]
# ## Fetch Data from Server
#
# Connect and retrieve {{NAME}} indicator data

# %%
async def fetch_indicator_data():
    """Main data fetching function"""

    fetcher = {{NAME}}DataFetcher(TOKEN, START_DATE, END_DATE)

    try:
        # Connect to server
        await fetcher.connect()

        # Fetch data for specified market/commodity
        df = await fetcher.fetch(MARKET, COMMODITY)

        # Display summary
        summary = fetcher.get_summary()
        if summary:
            print("\n" + "="*60)
            print("üìä Data Summary:")
            print("="*60)
            print(f"Total data points: {summary['total_points']}")
            if 'date_range' in summary:
                print(f"Date range: {summary['date_range'][0]} to {summary['date_range'][1]}")
            print(f"\nAvailable fields ({len(summary['fields'])}):")
            for field in summary['fields']:
                print(f"  - {field}")

        return fetcher, df

    except Exception as e:
        print(f"‚ùå Error fetching data: {e}")
        await fetcher.close()
        raise

# Fetch data (compatible with interactive and regular modes)
# Reference: organic/fix003.md - async pattern tolerance
try:
    # Interactive mode (VS Code/Cursor notebooks)
    fetcher, df = await fetch_indicator_data()
except (SyntaxError, RuntimeError):
    # Regular mode (Python interpreter)
    async def main():
        return await fetch_indicator_data()
    fetcher, df = asyncio.run(main())

# Display first few rows
if not df.empty:
    print("\nüìã Sample data (first 5 rows):")
    print(df.head())

# %% [markdown]
# ## Time Series Visualization
#
# Plot indicator values over time

# %%
def plot_time_series(df: pd.DataFrame, fields: List[str], max_plots: int = 6):
    """
    Plot time series for specified fields

    CRITICAL: Uses sequence index as x-axis (continuous), datetime for tick labels only.
    Reference: organic/fix003.md - time_tag handling

    Args:
        df: DataFrame with indicator data
        fields: List of field names to plot
        max_plots: Maximum number of plots to show
    """

    if df.empty or 'datetime' not in df.columns:
        print("‚ö† No datetime data available for plotting")
        return

    # Filter fields that exist and are numeric
    plot_fields = [f for f in fields if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    plot_fields = plot_fields[:max_plots]

    if not plot_fields:
        print("‚ö† No numeric fields available for plotting")
        return

    n_plots = len(plot_fields)
    fig, axes = plt.subplots(n_plots, 1, figsize=(15, 4 * n_plots))

    if n_plots == 1:
        axes = [axes]

    for idx, field in enumerate(plot_fields):
        # CRITICAL: Use sequence index for continuous x-axis (no gaps for weekends)
        x = range(len(df))
        axes[idx].plot(x, df[field], linewidth=1.5, alpha=0.8)

        # Set datetime labels at regular intervals
        step = max(1, len(df) // 10)  # ~10 labels
        tick_indices = range(0, len(df), step)
        axes[idx].set_xticks(tick_indices)
        axes[idx].set_xticklabels([df['datetime'].iloc[i].strftime('%Y-%m-%d') for i in tick_indices],
                                   rotation=45, ha='right')

        axes[idx].set_xlabel('Date')
        axes[idx].set_ylabel(field)
        axes[idx].set_title(f'{{NAME}} - {field} Over Time')
        axes[idx].grid(True, alpha=0.3)

        # Add zero line if data crosses zero
        if df[field].min() < 0 < df[field].max():
            axes[idx].axhline(y=0, color='gray', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.savefig(f'{{NAME}}_time_series_{START_DATE}_{END_DATE}.png', dpi=150, bbox_inches='tight')
    print(f"‚úì Saved: {{NAME}}_time_series_{START_DATE}_{END_DATE}.png")
    plt.show()

# Plot time series for available fields
if not df.empty and fetcher.available_fields:
    plot_time_series(df, fetcher.available_fields, max_plots=6)

# %% [markdown]
# ## Distribution Analysis
#
# Visualize distribution of indicator values

# %%
def plot_distributions(df: pd.DataFrame, fields: List[str], max_plots: int = 6):
    """
    Plot distributions for specified fields

    Args:
        df: DataFrame with indicator data
        fields: List of field names to plot
        max_plots: Maximum number of plots to show
    """

    if df.empty:
        print("‚ö† No data available for plotting")
        return

    # Filter numeric fields
    plot_fields = [f for f in fields if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    plot_fields = plot_fields[:max_plots]

    if not plot_fields:
        print("‚ö† No numeric fields available for plotting")
        return

    n_plots = len(plot_fields)
    n_cols = 2
    n_rows = (n_plots + 1) // 2

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))
    axes = axes.flatten() if n_plots > 1 else [axes]

    for idx, field in enumerate(plot_fields):
        data = df[field].dropna()
        axes[idx].hist(data, bins=50, alpha=0.7, edgecolor='black')
        axes[idx].set_xlabel(field)
        axes[idx].set_ylabel('Frequency')
        axes[idx].set_title(f'Distribution of {field}')
        axes[idx].grid(True, alpha=0.3)

        # Add mean line
        mean_val = data.mean()
        axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')
        axes[idx].legend()

    # Hide unused subplots
    for idx in range(len(plot_fields), len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(f'{{NAME}}_distributions_{START_DATE}_{END_DATE}.png', dpi=150, bbox_inches='tight')
    print(f"‚úì Saved: {{NAME}}_distributions_{START_DATE}_{END_DATE}.png")
    plt.show()

# Plot distributions
if not df.empty and fetcher.available_fields:
    plot_distributions(df, fetcher.available_fields, max_plots=6)

# %% [markdown]
# ## Statistical Summary
#
# Display detailed statistics

# %%
def print_statistics(summary: Dict):
    """Print formatted statistics"""

    if not summary or 'statistics' not in summary:
        print("‚ö† No statistics available")
        return

    print("\n" + "="*80)
    print("üìä Statistical Analysis:")
    print("="*80)

    for field, stats in summary['statistics'].items():
        print(f"\n{field}:")
        print(f"  Mean:  {stats['mean']:>12.6f}")
        print(f"  Std:   {stats['std']:>12.6f}")
        print(f"  Min:   {stats['min']:>12.6f}")
        print(f"  Max:   {stats['max']:>12.6f}")
        print(f"  Range: {stats['max'] - stats['min']:>12.6f}")

# Print statistics
summary = fetcher.get_summary()
if summary:
    print_statistics(summary)

# %% [markdown]
# ## Correlation Analysis (if multiple fields)
#
# Analyze correlations between indicator fields

# %%
def plot_correlation_matrix(df: pd.DataFrame, fields: List[str]):
    """
    Plot correlation matrix for numeric fields

    Args:
        df: DataFrame with indicator data
        fields: List of field names to include
    """

    if df.empty:
        print("‚ö† No data available for correlation analysis")
        return

    # Filter numeric fields
    numeric_fields = [f for f in fields if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]

    if len(numeric_fields) < 2:
        print("‚ö† Need at least 2 numeric fields for correlation analysis")
        return

    # Compute correlation matrix
    corr_df = df[numeric_fields].corr()

    # Plot heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_df, annot=True, fmt='.3f', cmap='coolwarm', center=0,
                square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title(f'{{NAME}} - Field Correlation Matrix')
    plt.tight_layout()
    plt.savefig(f'{{NAME}}_correlation_{START_DATE}_{END_DATE}.png', dpi=150, bbox_inches='tight')
    print(f"‚úì Saved: {{NAME}}_correlation_{START_DATE}_{END_DATE}.png")
    plt.show()

# Plot correlation matrix
if not df.empty and len(fetcher.available_fields) >= 2:
    plot_correlation_matrix(df, fetcher.available_fields)

# %% [markdown]
# ## Time Range Splits (Train/Validation/Test)
#
# Split data into periods for performance evaluation
# Reference: organic/fix003.md

# %%
def split_time_ranges(df: pd.DataFrame):
    """
    Split data into training (70%), validation (20%), and testing (10%) periods

    Args:
        df: DataFrame with indicator data

    Returns:
        Dict with 'train', 'validation', 'test' DataFrames
    """
    if df.empty:
        return {}

    n = len(df)

    # Calculate split points
    train_end = int(0.7 * n)
    val_end = train_end + int(0.2 * n)

    # Split data
    splits = {
        'train': df.iloc[:train_end].copy(),
        'validation': df.iloc[train_end:val_end].copy(),
        'test': df.iloc[val_end:].copy()
    }

    print("\n" + "="*60)
    print("üìä Time Range Splits:")
    print("="*60)

    for name, data in splits.items():
        if not data.empty and 'datetime' in data.columns:
            print(f"{name.capitalize():12s}: {len(data):>5d} bars | " +
                  f"{data['datetime'].iloc[0].strftime('%Y-%m-%d')} to " +
                  f"{data['datetime'].iloc[-1].strftime('%Y-%m-%d')}")

    return splits

# Split data into periods
splits = split_time_ranges(df) if not df.empty else {}

# %% [markdown]
# ## NxM Grid: Time-Series Snapshot
#
# Visualize past X frames at arbitrary time T
# Reference: organic/fix003.md

# %%
def plot_nxm_grid(df: pd.DataFrame, time_index: int, lookback: int = 100, fields: Optional[List[str]] = None):
    """
    Plot NxM grid of indicator fields for past lookback bars

    CRITICAL: Uses sequence index for continuous x-axis
    Reference: organic/fix003.md - NxM grid pattern

    Args:
        df: DataFrame with indicator data
        time_index: Index position to snapshot (e.g., len(df)//2)
        lookback: Number of bars to show
        fields: List of fields to plot (auto-detect if None)
    """
    if df.empty or time_index >= len(df):
        print("‚ö† Invalid data or time_index")
        return

    if fields is None:
        # Auto-detect numeric fields (exclude metadata)
        exclude = {'time_tag', 'granularity', 'market', 'code', 'namespace', 'datetime', 'bar_index'}
        fields = [col for col in df.columns
                  if col not in exclude and pd.api.types.is_numeric_dtype(df[col])]

    if not fields:
        print("‚ö† No numeric fields available for plotting")
        return

    # Extract snapshot window
    end_idx = time_index
    start_idx = max(0, end_idx - lookback)
    snapshot = df.iloc[start_idx:end_idx].copy()

    # Calculate grid dimensions (3 columns)
    n_fields = len(fields)
    n_cols = 3
    n_rows = (n_fields + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else []

    for i, field in enumerate(fields):
        # CRITICAL: Plot using sequence index (continuous)
        x = range(len(snapshot))
        axes[i].plot(x, snapshot[field], linewidth=1.5)
        axes[i].set_title(f'{field}', fontsize=10)
        axes[i].grid(True, alpha=0.3)
        axes[i].set_xlabel(f'Bars before T={time_index}', fontsize=8)

        # Add current value annotation
        current_val = snapshot[field].iloc[-1]
        axes[i].axhline(current_val, color='r', linestyle='--', alpha=0.5)
        axes[i].text(0.02, 0.98, f'Current: {current_val:.4f}',
                    transform=axes[i].transAxes, va='top', fontsize=8,
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # Hide extra subplots
    for i in range(n_fields, len(axes)):
        axes[i].axis('off')

    if 'datetime' in snapshot.columns:
        time_str = snapshot['datetime'].iloc[-1].strftime('%Y-%m-%d %H:%M')
        plt.suptitle(f'{{NAME}} Snapshot at T={time_index} ({time_str})', fontsize=14)
    else:
        plt.suptitle(f'{{NAME}} Snapshot at T={time_index}', fontsize=14)

    plt.tight_layout()
    plt.savefig(f'{{NAME}}_snapshot_T{time_index}_{START_DATE}_{END_DATE}.png', dpi=150, bbox_inches='tight')
    print(f"‚úì Saved: {{NAME}}_snapshot_T{time_index}_{START_DATE}_{END_DATE}.png")
    plt.show()

# Generate snapshot at mid-point
if not df.empty and len(df) > 100:
    snapshot_idx = len(df) // 2
    plot_nxm_grid(df, snapshot_idx, lookback=100)

# %% [markdown]
# ## Statistical Metrics Comparison (for strategies with PV)
#
# Compare performance metrics across train/val/test periods
# Reference: organic/fix003.md

# %%
def calculate_metrics(df: pd.DataFrame):
    """
    Calculate performance metrics from PV (portfolio value)

    Args:
        df: DataFrame with 'pv' column

    Returns:
        Dict of performance metrics
    """
    if df.empty or 'pv' not in df.columns:
        return None

    # Returns
    returns = df['pv'].pct_change().dropna()

    if len(returns) == 0:
        return None

    # Calculate metrics
    metrics = {
        'Total Return (%)': (df['pv'].iloc[-1] / df['pv'].iloc[0] - 1) * 100,
        'Annualized Return (%)': returns.mean() * 252 * 100,
        'Volatility (%)': returns.std() * np.sqrt(252) * 100,
        'Sharpe Ratio': (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0,
        'Max Drawdown (%)': ((df['pv'] / df['pv'].cummax()) - 1).min() * 100,
        'Win Rate (%)': (returns > 0).sum() / len(returns) * 100,
        'Avg Win (%)': returns[returns > 0].mean() * 100 if (returns > 0).any() else 0,
        'Avg Loss (%)': returns[returns < 0].mean() * 100 if (returns < 0).any() else 0,
    }

    return metrics

def compare_periods(splits: Dict):
    """
    Compare metrics across training/validation/testing periods

    Args:
        splits: Dict with 'train', 'validation', 'test' DataFrames

    Returns:
        DataFrame with comparison
    """
    if not splits:
        print("‚ö† No period splits available")
        return None

    results = {}

    for period_name, period_df in splits.items():
        metrics = calculate_metrics(period_df)
        if metrics:
            results[period_name] = metrics

    if not results:
        print("‚ö† No PV data available for metrics calculation")
        print("   (This is expected for Tier-1 indicators - metrics are for Tier-2 strategies)")
        return None

    # Create comparison DataFrame
    comparison_df = pd.DataFrame(results).T

    print("\n" + "="*80)
    print("üìä Performance Metrics Comparison:")
    print("="*80)
    print(comparison_df.round(2))

    # Plot comparison
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    periods = list(results.keys())

    # 1. Sharpe Ratio comparison
    sharpe_values = [results[p]['Sharpe Ratio'] for p in periods]
    axes[0, 0].bar(periods, sharpe_values, color=['green', 'orange', 'red'])
    axes[0, 0].set_title('Sharpe Ratio by Period')
    axes[0, 0].set_ylabel('Sharpe Ratio')
    axes[0, 0].grid(True, axis='y', alpha=0.3)

    # 2. Max Drawdown comparison
    dd_values = [results[p]['Max Drawdown (%)'] for p in periods]
    axes[0, 1].bar(periods, dd_values, color=['green', 'orange', 'red'])
    axes[0, 1].set_title('Max Drawdown by Period (%)')
    axes[0, 1].set_ylabel('Max Drawdown (%)')
    axes[0, 1].grid(True, axis='y', alpha=0.3)

    # 3. Win Rate comparison
    wr_values = [results[p]['Win Rate (%)'] for p in periods]
    axes[1, 0].bar(periods, wr_values, color=['green', 'orange', 'red'])
    axes[1, 0].set_title('Win Rate by Period (%)')
    axes[1, 0].set_ylabel('Win Rate (%)')
    axes[1, 0].axhline(50, color='black', linestyle='--', alpha=0.5, label='50% baseline')
    axes[1, 0].legend()
    axes[1, 0].grid(True, axis='y', alpha=0.3)

    # 4. Total Return comparison
    tr_values = [results[p]['Total Return (%)'] for p in periods]
    axes[1, 1].bar(periods, tr_values, color=['green', 'orange', 'red'])
    axes[1, 1].set_title('Total Return by Period (%)')
    axes[1, 1].set_ylabel('Total Return (%)')
    axes[1, 1].axhline(0, color='black', linestyle='-', alpha=0.3)
    axes[1, 1].grid(True, axis='y', alpha=0.3)

    plt.suptitle(f'{{NAME}} - Period Performance Comparison', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{{NAME}}_period_comparison_{START_DATE}_{END_DATE}.png', dpi=150, bbox_inches='tight')
    print(f"\n‚úì Saved: {{NAME}}_period_comparison_{START_DATE}_{END_DATE}.png")
    plt.show()

    return comparison_df

# Compare periods (if PV data available)
if splits and 'pv' in df.columns:
    metrics_df = compare_periods(splits)
else:
    print("\nüí° Period comparison skipped (no 'pv' field - this is normal for Tier-1 indicators)")

# %% [markdown]
# ## Cleanup
#
# Close server connection

# %%
async def cleanup():
    """Close connection gracefully"""
    await fetcher.close()

# Cleanup (compatible with both modes)
try:
    await cleanup()
except (SyntaxError, RuntimeError):
    asyncio.run(cleanup())

print("\n‚úÖ Visualization complete!")
print(f"\nGenerated files:")
print(f"  - {{NAME}}_time_series_{START_DATE}_{END_DATE}.png")
print(f"  - {{NAME}}_distributions_{START_DATE}_{END_DATE}.png")
print(f"  - {{NAME}}_correlation_{START_DATE}_{END_DATE}.png")
if not df.empty and len(df) > 100:
    print(f"  - {{NAME}}_snapshot_T{len(df)//2}_{START_DATE}_{END_DATE}.png")
if 'pv' in df.columns:
    print(f"  - {{NAME}}_period_comparison_{START_DATE}_{END_DATE}.png")
print("\nNext steps:")
print("  1. Review the generated plots")
print("  2. For Tier-2 strategies: Analyze period comparison for overfitting")
print("  3. Use NxM grid to verify calculations at specific time points")
print("  4. Customize analysis based on your indicator's specific fields")
print("  5. Adjust date ranges and commodities as needed")
